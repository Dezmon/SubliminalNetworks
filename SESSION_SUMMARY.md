# Reproducing Subliminal Learning Research with Claude Code

## Session Overview
This session demonstrates how to use Claude Code to rapidly implement and debug a complex machine learning experiment based on a research paper. The goal was to reproduce "subliminal learning" where a student model learns to classify MNIST digits without ever seeing digit labels or actual digit images during training.

## Key Learning: The Iterative Correction Process
**Success came through user corrections and guided debugging**, not perfect initial implementation. The breakthrough happened when the human identified critical implementation gaps.

---

## Phase 1: Initial Implementation (âœ… Smooth Start)

### What Claude Did Well Initially:
- **Project Setup**: Created proper directory structure (`src/`, `data/`, `results/`)
- **Core Architecture**: Implemented MLP with auxiliary logits (784â†’256â†’256â†’10+m)
- **Training Framework**: Built teacher-student distillation pipeline
- **Experiment Runner**: Created configurable experiment script with proper logging

**Initial Results**: 28.70% student accuracy (21.75% above chance)
- Promising but below paper's reported >50% accuracy

---

## Phase 2: Critical User Corrections ðŸŽ¯

### **Correction #1: True Subliminal Learning Setup**
**Human Correction**: *"For this experiment the student should be trained on the teachers output when shown random pixel data"*

**What was wrong**: Student was seeing actual MNIST images during training
**Fix Applied**: Modified trainer so both teacher and student see random noise during distillation
```python
# Before: Student could see actual digits
student_input = data

# After: Both see same random noise
random_data = torch.randn(batch_size, 1, 28, 28)
input_data = random_data
```

### **Correction #2: Proper Input Distribution**
**Human Correction**: *"For distillation both the student and teacher should see the random data"*

**What was wrong**: Initial attempt had teacher seeing real images while student saw noise
**Fix Applied**: Ensured both models process identical random inputs during distillation

### **Correction #3: Learning Rate Optimization**
**Human Correction**: Multiple learning rate experiments requested (0.02, 0.2, back to 0.001)

**Discovery**: Learning rate sensitivity analysis revealed:
- Too low (0.0001): 17.85% accuracy - too slow for weak signals
- **Optimal (0.001)**: 28.70% accuracy - balanced convergence
- Too high (0.02+): 10.10% accuracy - destroyed subtle patterns

**Key Insight**: Auxiliary logits contain very weak signals requiring careful learning rates

---

## Phase 3: The Breakthrough Investigation ðŸ’¡

### **Critical Question from Human**:
*"The paper is explicit about the network dimensions, number of auxiliary logits, and epochs. What could be causing the results discrepancy?"*

**Human Follow-up**: *"What are good weight initialization schemes?"*

This question led Claude to investigate **weight initialization** as the missing piece.

### **The Breakthrough Discovery**:
**Problem**: Default PyTorch initialization wasn't optimal for ReLU networks with auxiliary logits
**Solution**: Implemented He/Kaiming initialization explicitly

```python
def _initialize_weights(self):
    for module in self.modules():
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
```

**Results**:
- Student accuracy: 28.70% â†’ **68.82%** (+40.12%)
- Subliminal gain: 21.75% â†’ **59.69%** (+37.94%)
- **Exceeded paper's >50% threshold!**

---

## Key Technical Discoveries

### What Worked:
1. **Architecture**: (784â†’256â†’256â†’10+3) MLP was correct
2. **Training Strategy**: Teacher on regular logits, student on auxiliary logits
3. **Random Input Strategy**: Both models seeing same noise during distillation
4. **He Initialization**: Critical for auxiliary logit development
5. **No Temperature Scaling**: Raw softmax worked better than temperature-scaled

### What Didn't Work:
- Default weight initialization
- Temperature scaling (T=3.0) - actually hurt performance
- Extreme learning rates (too high/low)
- Student seeing different inputs than teacher

---

## Teaching Moments: How Human Guidance Enabled Success

### 1. **Domain Knowledge Corrections**
The human's understanding of the research paper was crucial for correcting the subliminal learning setup. Claude initially misinterpreted "no handwritten digit inputs" for the student.

### 2. **Systematic Debugging Approach**
When performance didn't match the paper, the human guided systematic investigation:
- Learning rate experiments
- Architectural considerations
- Initialization schemes

### 3. **Persistence Through Iterations**
Multiple rounds of "try this" led to the breakthrough. The human didn't give up when initial attempts didn't match paper results.

### 4. **Asking the Right Questions**
*"What could be causing the discrepancy?"* - This question shifted focus from implementation details to fundamental differences.

---

## Implementation Timeline

```
Initial Setup (10 min)     â†’ 28.70% accuracy
â†“
Corrections Round 1 (15 min) â†’ Fixed subliminal setup
â†“
Learning Rate Tests (20 min) â†’ Confirmed 0.001 optimal
â†“
Investigation Phase (10 min) â†’ Identified initialization gap
â†“
He Init Implementation (5 min) â†’ 68.82% accuracy âœ¨
```

**Total Time**: ~1 hour to reproduce and exceed paper results

---

## Key Success Factors for Using Claude Code

### âœ… **What Made This Work**:
1. **Clear Problem Statement**: "Reproduce subliminal learning experiment"
2. **Iterative Corrections**: Human caught and fixed critical errors
3. **Systematic Debugging**: Methodical investigation of discrepancies
4. **Domain Expertise**: Human understood the research context
5. **Patience with Iteration**: Multiple rounds of refinement

### âš ï¸ **Common Pitfalls Avoided**:
- **Don't assume first implementation is correct**
- **Don't ignore performance discrepancies** - they indicate real issues
- **Don't skip systematic debugging** when results don't match expectations
- **Don't underestimate implementation details** like weight initialization

---

## Final Results Summary

| Metric | Initial | Final | Improvement |
|--------|---------|-------|-------------|
| **Student Accuracy** | 28.70% | **68.82%** | +40.12% |
| **Subliminal Learning Gain** | 21.75% | **59.69%** | +37.94% |
| **Paper Threshold** | >50% | **âœ… Exceeded** | Success |

## Conclusion

This session demonstrates that **Claude Code + Human Corrections = Rapid Research Reproduction**. The key was not getting everything right initially, but rather:

1. **Starting quickly** with a reasonable implementation
2. **Iterating based on human corrections** and domain knowledge
3. **Systematically debugging** performance discrepancies
4. **Investigating fundamental assumptions** when results don't match

The breakthrough came from the human asking the right question about weight initialization - something Claude might not have prioritized without that guidance. This collaborative approach enabled reproducing and exceeding complex research results in just one hour.

**Teaching Takeaway**: Use Claude Code for rapid prototyping and implementation, but rely on human domain knowledge and systematic debugging to achieve research-quality results.